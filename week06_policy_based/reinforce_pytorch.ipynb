{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE in pytorch\n",
    "\n",
    "Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n",
    "\n",
    "Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # in google colab uncomment this\n",
    "\n",
    "# import os\n",
    "\n",
    "# os.system('apt-get install -y xvfb')\n",
    "# os.system('wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb')\n",
    "# os.system('apt-get install -y python-opengl ffmpeg')\n",
    "# os.system('pip install pyglet==1.2.4')\n",
    "\n",
    "# os.system('python -m pip install -U pygame --user')\n",
    "\n",
    "# print('setup complete')\n",
    "\n",
    "# XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsur/HDD/Github/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fb30cb9b610>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network for REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple neural network that predicts policy logits. \n",
    "# Keep it simple: CartPole isn't worth deep architectures.\n",
    "model = nn.Sequential(nn.Linear(4, 100),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(100, 2)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1814, 0.0304]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = Variable(torch.FloatTensor(env.reset().reshape([1,4])))\n",
    "\n",
    "model.forward(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.cuda.device at 0x7fb30cb93510>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(states):\n",
    "    \"\"\" \n",
    "    Predict action probabilities given states.\n",
    "    :param states: numpy array of shape [batch, state_shape]\n",
    "    :returns: numpy array of shape [batch, n_actions]\n",
    "    \"\"\"\n",
    "    # convert states, compute logits, use softmax to get probability\n",
    "    states = Variable(torch.FloatTensor(states))\n",
    "    probas = F.softmax(model.forward(states))\n",
    "    return probas.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsur/anaconda3/envs/rl/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_probs(test_states)\n",
    "assert isinstance(\n",
    "    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
    "assert tuple(test_probas.shape) == (\n",
    "    test_states.shape[0], env.action_space.n), \"wrong output shape: %s\" % np.shape(test_probas)\n",
    "assert np.allclose(np.sum(test_probas, axis=1),\n",
    "                   1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play the game\n",
    "\n",
    "We can now use our newly built agent to play the game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000):\n",
    "    \"\"\" \n",
    "    play a full session with REINFORCE agent and train at the session end.\n",
    "    returns sequences of states, actions andrewards\n",
    "    \"\"\"\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probs = predict_probs(np.array([s]))[0]\n",
    "\n",
    "        # Sample action with given probabilities.\n",
    "        a = np.random.choice(n_actions, p=action_probs)\n",
    "        new_s, r, done, info = env.step(a)\n",
    "\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return states, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsur/anaconda3/envs/rl/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# test it\n",
    "states, actions, rewards = generate_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards,  # rewards at each step\n",
    "                           gamma=0.99  # discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n",
    "    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "\n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute G_t = r_t + gamma*G_{t+1} recurrently\n",
    "\n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    G = [rewards[-1]]\n",
    "    \n",
    "    for r in rewards[-2::-1]:\n",
    "        G.append(r + gamma * G[-1])\n",
    "\n",
    "    return G[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looks good!\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n",
    "                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards(\n",
    "    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "We now need to define objective and update over policy gradient.\n",
    "\n",
    "Our objective function is\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Following the REINFORCE algorithm, we can define our objective as follows: \n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, ndims):\n",
    "    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    y_one_hot = torch.zeros(\n",
    "        y_tensor.size()[0], ndims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code: define optimizers\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "\n",
    "def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n",
    "    \"\"\"\n",
    "    Takes a sequence of states, actions and rewards produced by generate_session.\n",
    "    Updates agent's weights by following the policy gradient above.\n",
    "    Please use Adam optimizer with default parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # cast everything into torch tensors\n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "\n",
    "    # predict logits, probas and log-probas using an agent.\n",
    "    logits = model(states)\n",
    "    probs = nn.functional.softmax(logits, -1)\n",
    "    log_probs = nn.functional.log_softmax(logits, -1)\n",
    "\n",
    "    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n",
    "        \"please use compute using torch tensors and don't use predict_probs function\"\n",
    "\n",
    "    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n",
    "    log_probs_for_actions = torch.sum(\n",
    "        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n",
    "   \n",
    "    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n",
    "    entropy = -(probs * log_probs).sum(-1).mean()\n",
    "    loss = -torch.mean(log_probs_for_actions * cumulative_returns) - entropy_coef * entropy\n",
    "\n",
    "    # Gradient descent step\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # technical: return session rewards to print them later\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsur/anaconda3/envs/rl/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:22.600\n",
      "mean reward:28.170\n",
      "mean reward:41.490\n",
      "mean reward:72.710\n",
      "mean reward:140.950\n",
      "mean reward:159.590\n",
      "mean reward:225.800\n",
      "mean reward:325.300\n",
      "mean reward:218.020\n",
      "mean reward:156.650\n",
      "mean reward:141.570\n",
      "mean reward:134.830\n",
      "mean reward:171.260\n",
      "mean reward:172.110\n",
      "mean reward:109.680\n",
      "mean reward:125.970\n",
      "mean reward:173.970\n",
      "mean reward:647.470\n",
      "You Win!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session())\n",
    "               for _ in range(100)]  # generate new sessions\n",
    "    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n",
    "    if np.mean(rewards) > 500:\n",
    "        print(\"You Win!\")  # but you can train even further\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zsur/HDD/Github/gym/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/zsur/anaconda3/envs/rl/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n",
      "/home/zsur/anaconda3/envs/rl/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n",
      "/home/zsur/anaconda3/envs/rl/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n",
      "/home/zsur/anaconda3/envs/rl/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n",
      "/home/zsur/anaconda3/envs/rl/lib/python3.7/site-packages/ipykernel_launcher.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),\n",
    "                           directory=\"videos\", force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.22144.video000027.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(\n",
    "    filter(lambda s: s.endswith(\".mp4\"), os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))  # this may or may not be the _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
